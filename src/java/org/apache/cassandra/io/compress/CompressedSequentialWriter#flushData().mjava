    @Override
    protected void flushData()
    {
        seekToChunkStart(); // why is this necessary? seems like it should always be at chunk start in normal operation

        int compressedLength;
        try
        {
            // compressing data with buffer re-use
            buffer.flip();
            compressed.buffer.clear();
            compressedLength = compressor.compress(buffer, compressed);

            // Compressors don't modify sentinels in our BB - we rely on buffer.position() for bufferOffset adjustment
            buffer.position(buffer.limit());
        }
        catch (IOException e)
        {
            throw new RuntimeException("Compression exception", e); // shouldn't happen
        }

        uncompressedSize += buffer.position();
        compressedSize += compressedLength;

        try
        {
            // write an offset of the newly written chunk to the index file
            metadataWriter.addOffset(chunkOffset);
            chunkCount++;

            assert compressedLength <= compressed.buffer.capacity();

            // write out the compressed data
            compressed.buffer.flip();
            channel.write(compressed.buffer);

            // write corresponding checksum
            compressed.buffer.rewind();
            crcMetadata.appendDirect(compressed.buffer);
            lastFlushOffset += compressedLength + 4;

            // adjust our bufferOffset to account for the new uncompressed data we've now written out
            resetBuffer();
        }
        catch (IOException e)
        {
            throw new FSWriteError(e, getPath());
        }

        // next chunk should be written right after current + length of the checksum (int)
        chunkOffset += compressedLength + 4;
        if (runPostFlush != null)
            runPostFlush.run();
    }

