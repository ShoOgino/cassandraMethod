    /*
     * This function does the actual compaction for files.
     * It maintains a priority queue of with the first key from each file
     * and then removes the top of the queue and adds it to the SStable and
     * repeats this process while reading the next from each file until its
     * done with all the files . The SStable to which the keys are written
     * represents the new compacted file. Before writing if there are keys
     * that occur in multiple files and are the same then a resolution is done
     * to get the latest data.
     *
     */
    void  doFileCompaction(List<String> files,  int minBufferSize) throws IOException
    {
    	String newfile = null;
        long startTime = System.currentTimeMillis();
        long totalBytesRead = 0;
        long totalBytesWritten = 0;
        long totalkeysRead = 0;
        long totalkeysWritten = 0;
        try
        {
	        // Calculate the expected compacted filesize
	    	long expectedCompactedFileSize = getExpectedCompactedFileSize(files);
	        String compactionFileLocation = DatabaseDescriptor.getCompactionFileLocation(expectedCompactedFileSize);
	        // If the compaction file path is null that means we have no space left for this compaction.
	        if( compactionFileLocation == null )
	        {
        		String maxFile = getMaxSizeFile( files );
        		files.remove( maxFile );
        		doFileCompaction(files , minBufferSize);
        		return;
	        }
	        PriorityQueue<FileStruct> pq = initializePriorityQueue(files, null, minBufferSize);
	        if (pq.size() > 0)
	        {
	            String mergedFileName = getTempFileName( files );
	            SSTable ssTable = null;
	            String lastkey = null;
	            List<FileStruct> lfs = new ArrayList<FileStruct>();
	            DataOutputBuffer bufOut = new DataOutputBuffer();
	            int expectedBloomFilterSize = SSTable.getApproximateKeyCount(files);
	            expectedBloomFilterSize = (expectedBloomFilterSize > 0) ? expectedBloomFilterSize : SSTable.indexInterval();
	            logger_.debug("Expected bloom filter size : " + expectedBloomFilterSize);
	            /* Create the bloom filter for the compacted file. */
	            BloomFilter compactedBloomFilter = new BloomFilter(expectedBloomFilterSize, 15);
	            List<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>();

	            while (pq.size() > 0 || lfs.size() > 0)
	            {
	                FileStruct fs = null;
	                if (pq.size() > 0)
	                {
	                    fs = pq.poll();                        
	                }
	                if (fs != null
	                        && (lastkey == null || lastkey.compareTo(fs.key_) == 0))
	                {
	                    // The keys are the same so we need to add this to the
	                    // ldfs list
	                    lastkey = fs.key_;
	                    lfs.add(fs);
	                }
	                else
	                {
	                    Collections.sort(lfs, new FileStructComparator());
	                    ColumnFamily columnFamily = null;
	                    bufOut.reset();
	                    if(lfs.size() > 1)
	                    {
		                    for (FileStruct filestruct : lfs)
		                    {
		                    	try
		                    	{
	                                /* read the length although we don't need it */
	                                filestruct.bufIn_.readInt();
	                                // Skip the Index
                                    IndexHelper.skipBloomFilterAndIndex(filestruct.bufIn_);
	                                // We want to add only 2 and resolve them right there in order to save on memory footprint
	                                if(columnFamilies.size() > 1)
	                                {
	    		                        // Now merge the 2 column families
	    			                    columnFamily = resolve(columnFamilies);
	    			                    columnFamilies.clear();
	    			                    if( columnFamily != null)
	    			                    {
		    			                    // add the merged columnfamily back to the list
		    			                    columnFamilies.add(columnFamily);
	    			                    }

	                                }
			                        // deserialize into column families                                    
			                        columnFamilies.add(ColumnFamily.serializer().deserialize(filestruct.bufIn_));
		                    	}
		                    	catch ( Exception ex)
		                    	{                                    		                    		
		                            continue;
		                    	}
		                    }
		                    // Now after merging all crap append to the sstable
		                    columnFamily = resolve(columnFamilies);
		                    columnFamilies.clear();
		                    if( columnFamily != null )
		                    {
			                	/* serialize the cf with column indexes */
			                    ColumnFamily.serializerWithIndexes().serialize(columnFamily, bufOut);
		                    }
	                    }
	                    else
	                    {
		                    FileStruct filestruct = lfs.get(0);
	                    	try
	                    	{
		                        /* read the length although we don't need it */
		                        int size = filestruct.bufIn_.readInt();
		                        bufOut.write(filestruct.bufIn_, size);
	                    	}
	                    	catch ( Exception ex)
	                    	{
	                    		ex.printStackTrace();
	                            filestruct.reader_.close();
	                            continue;
	                    	}
	                    }
	                    	         
	                    if ( ssTable == null )
	                    {
	                    	PartitionerType pType = StorageService.getPartitionerType();
	                    	ssTable = new SSTable(compactionFileLocation, mergedFileName, pType);	                    	
	                    }
	                    doWrite(ssTable, lastkey, bufOut);	                 
	                    
                        /* Fill the bloom filter with the key */
	                    doFill(compactedBloomFilter, lastkey);                        
	                    totalkeysWritten++;
	                    for (FileStruct filestruct : lfs)
	                    {
	                    	try
	                    	{
	                    		filestruct = getNextKey(filestruct);
	                    		if(filestruct == null)
	                    		{
	                    			continue;
	                    		}
	                    		pq.add(filestruct);
		                        totalkeysRead++;
	                    	}
	                    	catch ( Throwable ex )
	                    	{
	                    		// Ignore the exception as it might be a corrupted file
	                    		// in any case we have read as far as possible from it
	                    		// and it will be deleted after compaction.
	                            filestruct.reader_.close();
	                            continue;
	                    	}
	                    }
	                    lfs.clear();
	                    lastkey = null;
	                    if (fs != null)
	                    {
	                        /* Add back the fs since we processed the rest of filestructs */
	                        pq.add(fs);
	                    }
	                }
	            }
	            if ( ssTable != null )
	            {
	                ssTable.closeRename(compactedBloomFilter);
	                newfile = ssTable.getDataFileLocation();
	            }
	            lock_.writeLock().lock();
	            try
	            {
	                for (String file : files)
	                {
	                    ssTables_.remove(file);
	                    SSTable.removeAssociatedBloomFilter(file);
	                }
	                if ( newfile != null )
	                {
	                    ssTables_.add(newfile);
	                    logger_.debug("Inserting bloom filter for file " + newfile);
	                    SSTable.storeBloomFilter(newfile, compactedBloomFilter);
	                    totalBytesWritten = (new File(newfile)).length();
	                }
	            }
	            finally
	            {
	                lock_.writeLock().unlock();
	            }
	            for (String file : files)
	            {
	                SSTable.delete(file);
	            }
	        }
        }
        catch ( Exception ex)
        {
            logger_.warn( LogUtil.throwableToString(ex) );
        }
        logger_.debug("Total time taken for compaction  ..."
                + (System.currentTimeMillis() - startTime));
        logger_.debug("Total bytes Read for compaction  ..." + totalBytesRead);
        logger_.debug("Total bytes written for compaction  ..."
                + totalBytesWritten + "   Total keys read ..." + totalkeysRead);
        return;
    }

