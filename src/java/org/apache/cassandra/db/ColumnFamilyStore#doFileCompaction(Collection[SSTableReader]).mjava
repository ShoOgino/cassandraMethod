    /*
    * This function does the actual compaction for files.
    * It maintains a priority queue of with the first key from each file
    * and then removes the top of the queue and adds it to the SStable and
    * repeats this process while reading the next from each file until its
    * done with all the files . The SStable to which the keys are written
    * represents the new compacted file. Before writing if there are keys
    * that occur in multiple files and are the same then a resolution is done
    * to get the latest data.
    *
    */
    private int doFileCompaction(Collection<SSTableReader> sstables) throws IOException
    {
        logger_.info("Compacting [" + StringUtils.join(sstables, ",") + "]");
        String compactionFileLocation = DatabaseDescriptor.getDataFileLocationForTable(table_, getExpectedCompactedFileSize(sstables));
        // If the compaction file path is null that means we have no space left for this compaction.
        // try again w/o the largest one.
        if (compactionFileLocation == null)
        {
            SSTableReader maxFile = getMaxSizeFile(sstables);
            sstables.remove(maxFile);
            return doFileCompaction(sstables);
        }

        long startTime = System.currentTimeMillis();
        long totalBytesRead = 0;
        long totalkeysRead = 0;
        long totalkeysWritten = 0;
        PriorityQueue<FileStruct> pq = initializePriorityQueue(sstables, null);

        if (pq.isEmpty())
        {
            logger_.warn("Nothing to compact (all files empty or corrupt). This should not happen.");
            // TODO clean out bad files, if any
            return 0;
        }

        int expectedBloomFilterSize = SSTableReader.getApproximateKeyCount(sstables);
        if (expectedBloomFilterSize < 0)
            expectedBloomFilterSize = SSTableReader.indexInterval();
        String newFilename = new File(compactionFileLocation, getTempSSTableFileName()).getAbsolutePath();
        SSTableWriter writer = new SSTableWriter(newFilename, expectedBloomFilterSize, StorageService.getPartitioner());
        SSTableReader ssTable = null;
        String lastkey = null;
        List<FileStruct> lfs = new ArrayList<FileStruct>();
        DataOutputBuffer bufOut = new DataOutputBuffer();
        if (logger_.isDebugEnabled())
          logger_.debug("Expected bloom filter size : " + expectedBloomFilterSize);
        List<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>();

        while (pq.size() > 0 || lfs.size() > 0)
        {
            FileStruct fs = null;
            if (pq.size() > 0)
            {
                fs = pq.poll();
            }
            if (fs != null
                && (lastkey == null || lastkey.equals(fs.getKey())))
            {
                // The keys are the same so we need to add this to the
                // ldfs list
                lastkey = fs.getKey();
                lfs.add(fs);
            }
            else
            {
                Collections.sort(lfs, new FileStructComparator());
                ColumnFamily columnFamily;
                bufOut.reset();
                if (lfs.size() > 1)
                {
                    for (FileStruct filestruct : lfs)
                    {
                        // We want to add only 2 and resolve them right there in order to save on memory footprint
                        if (columnFamilies.size() > 1)
                        {
                            merge(columnFamilies);
                        }
                        // deserialize into column families
                        columnFamilies.add(filestruct.getColumnFamily());
                    }
                    // Now after merging all crap append to the sstable
                    columnFamily = resolveAndRemoveDeleted(columnFamilies);
                    columnFamilies.clear();
                    if (columnFamily != null)
                    {
                        ColumnFamily.serializer().serializeWithIndexes(columnFamily, bufOut);
                    }
                }
                else
                {
                    // TODO deserializing only to reserialize is dumb
                    FileStruct filestruct = lfs.get(0);
                    ColumnFamily.serializer().serializeWithIndexes(filestruct.getColumnFamily(), bufOut);
                }

                writer.append(lastkey, bufOut);
                totalkeysWritten++;

                for (FileStruct filestruct : lfs)
                {
                    filestruct.advance(true);
                    if (filestruct.isExhausted())
                    {
                        continue;
                    }
                    pq.add(filestruct);
                    totalkeysRead++;
                }
                lfs.clear();
                lastkey = null;
                if (fs != null)
                {
                    /* Add back the fs since we processed the rest of filestructs */
                    pq.add(fs);
                }
            }
        }
        ssTable = writer.closeAndOpenReader();
        ssTables_.add(ssTable);
        ssTables_.markCompacted(sstables);
        MinorCompactionManager.instance().submit(ColumnFamilyStore.this);

        String format = "Compacted to %s.  %d/%d bytes for %d/%d keys read/written.  Time: %dms.";
        long dTime = System.currentTimeMillis() - startTime;
        logger_.info(String.format(format, writer.getFilename(), totalBytesRead, ssTable.length(), totalkeysRead, totalkeysWritten, dTime));
        return sstables.size();
    }

