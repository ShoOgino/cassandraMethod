    /**
     * This function is used to do the anti compaction process , it spits out the file which has keys that belong to a given range
     * If the target is not specified it spits out the file as a compacted file with the unecessary ranges wiped out.
     *
     * @param files
     * @param ranges
     * @param target
     * @param fileList
     * @return
     * @throws IOException
     */
    boolean doFileAntiCompaction(List<String> files, List<Range> ranges, EndPoint target, List<String> fileList) throws IOException
    {
        boolean result = false;
        long startTime = System.currentTimeMillis();
        long totalBytesRead = 0;
        long totalBytesWritten = 0;
        long totalkeysRead = 0;
        long totalkeysWritten = 0;
        String rangeFileLocation;
        String mergedFileName;
        // Calculate the expected compacted filesize
        long expectedRangeFileSize = getExpectedCompactedFileSize(files);
        /* in the worst case a node will be giving out half of its data so we take a chance */
        expectedRangeFileSize = expectedRangeFileSize / 2;
        rangeFileLocation = DatabaseDescriptor.getDataFileLocationForTable(table_, expectedRangeFileSize);
        // If the compaction file path is null that means we have no space left for this compaction.
        if (rangeFileLocation == null)
        {
            logger_.error("Total bytes to be written for range compaction  ..."
                          + expectedRangeFileSize + "   is greater than the safe limit of the disk space available.");
            return result;
        }
        PriorityQueue<FileStruct> pq = initializePriorityQueue(files, ranges);
        if (pq.isEmpty())
        {
            return result;
        }

        mergedFileName = getTempSSTableFileName();
        SSTableWriter rangeWriter = null;
        String lastkey = null;
        List<FileStruct> lfs = new ArrayList<FileStruct>();
        DataOutputBuffer bufOut = new DataOutputBuffer();
        int expectedBloomFilterSize = SSTableReader.getApproximateKeyCount(files);
        expectedBloomFilterSize = (expectedBloomFilterSize > 0) ? expectedBloomFilterSize : SSTableReader.indexInterval();
        if (logger_.isDebugEnabled())
          logger_.debug("Expected bloom filter size : " + expectedBloomFilterSize);
        List<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>();

        while (pq.size() > 0 || lfs.size() > 0)
        {
            FileStruct fs = null;
            if (pq.size() > 0)
            {
                fs = pq.poll();
            }
            if (fs != null
                && (lastkey == null || lastkey.equals(fs.getKey())))
            {
                // The keys are the same so we need to add this to the
                // ldfs list
                lastkey = fs.getKey();
                lfs.add(fs);
            }
            else
            {
                Collections.sort(lfs, new FileStructComparator());
                ColumnFamily columnFamily;
                bufOut.reset();
                if (lfs.size() > 1)
                {
                    for (FileStruct filestruct : lfs)
                    {
                        // We want to add only 2 and resolve them right there in order to save on memory footprint
                        if (columnFamilies.size() > 1)
                        {
                            // Now merge the 2 column families
                            merge(columnFamilies);
                        }
                        // deserialize into column families
                        columnFamilies.add(filestruct.getColumnFamily());
                    }
                    // Now after merging all crap append to the sstable
                    columnFamily = resolveAndRemoveDeleted(columnFamilies);
                    columnFamilies.clear();
                    if (columnFamily != null)
                    {
                        ColumnFamily.serializerWithIndexes().serialize(columnFamily, bufOut);
                    }
                }
                else
                {
                    // TODO deserializing only to reserialize is dumb
                    FileStruct filestruct = lfs.get(0);
                    ColumnFamily.serializerWithIndexes().serialize(filestruct.getColumnFamily(), bufOut);
                }
                if (Range.isTokenInRanges(StorageService.getPartitioner().getInitialToken(lastkey), ranges))
                {
                    if (rangeWriter == null)
                    {
                        if (target != null)
                        {
                            rangeFileLocation = rangeFileLocation + File.separator + "bootstrap";
                        }
                        FileUtils.createDirectory(rangeFileLocation);
                        String fname = new File(rangeFileLocation, mergedFileName).getAbsolutePath();
                        rangeWriter = new SSTableWriter(fname, expectedBloomFilterSize, StorageService.getPartitioner());
                    }
                    rangeWriter.append(lastkey, bufOut);
                }
                totalkeysWritten++;
                for (FileStruct filestruct : lfs)
                {
                    filestruct.advance(true);
                    if (filestruct.isExhausted())
                    {
                        continue;
                    }
                    /* keep on looping until we find a key in the range */
                    while (!Range.isTokenInRanges(StorageService.getPartitioner().getInitialToken(filestruct.getKey()), ranges))
                    {
                        filestruct.advance(true);
                        if (filestruct.isExhausted())
                        {
                            break;
                        }
                    }
                    if (!filestruct.isExhausted())
                    {
                        pq.add(filestruct);
                    }
                    totalkeysRead++;
                }
                lfs.clear();
                lastkey = null;
                if (fs != null)
                {
                    // Add back the fs since we processed the rest of
                    // filestructs
                    pq.add(fs);
                }
            }
        }

        if (rangeWriter != null)
        {
            rangeWriter.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_));
            if (fileList != null)
            {
                //Retain order. The -Data.db file needs to be last because 
                //the receiving end checks for this file before opening the SSTable
                //and adding this to the list of SSTables.
                fileList.add(rangeWriter.indexFilename());
                fileList.add(rangeWriter.filterFilename());
                fileList.add(rangeWriter.getFilename());
            }
            result = true;
        }

        if (logger_.isDebugEnabled())
        {
            logger_.debug("Total time taken for range split   ..." + (System.currentTimeMillis() - startTime));
            logger_.debug("Total bytes Read for range split  ..." + totalBytesRead);
            logger_.debug("Total bytes written for range split  ..."
                          + totalBytesWritten + "   Total keys read ..." + totalkeysRead);
        }
        return result;
    }

