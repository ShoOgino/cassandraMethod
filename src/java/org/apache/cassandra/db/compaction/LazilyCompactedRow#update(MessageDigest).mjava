    public void update(final MessageDigest digest)
    {
        assert !closed;

        // no special-case for rows.size == 1, we're actually skipping some bytes here so just
        // blindly updating everything wouldn't be correct
        DataOutputBuffer out = new DataOutputBuffer();
        OnDiskAtom.SerializerForWriting serializer = new OnDiskAtom.SerializerForWriting()
        {
            @Override
            public void serializeForSSTable(OnDiskAtom atom, DataOutputPlus out) throws IOException
            {
                atom.updateDigest(digest);
            }

            @Override
            public long serializedSizeForSSTable(OnDiskAtom atom)
            {
                return 0;
            }
        };

        // initialize indexBuilder for the benefit of its tombstoneTracker, used by our reducing iterator
        indexBuilder = new ColumnIndex.Builder(emptyColumnFamily, key.getKey(), out, serializer);

        try
        {
            DeletionTime.serializer.serialize(emptyColumnFamily.deletionInfo().getTopLevelDeletion(), out);

            // do not update digest in case of missing or purged row level tombstones, see CASSANDRA-8979
            // - digest for non-empty rows needs to be updated with deletion in any case to match digest with versions before patch
            // - empty rows must not update digest in case of LIVE delete status to avoid mismatches with non-existing rows
            //   this will however introduce in return a digest mismatch for versions before patch (which would update digest in any case)
            if (merger.hasNext() || emptyColumnFamily.deletionInfo().getTopLevelDeletion() != DeletionTime.LIVE)
            {
                digest.update(out.getData(), 0, out.getLength());
            }
            indexBuilder.buildForCompaction(merger);
        }
        catch (IOException e)
        {
            throw new AssertionError(e);
        }

        close();
    }

